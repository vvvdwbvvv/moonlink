use super::data_batches::{create_batch_from_rows, InMemoryBatch};
use super::delete_vector::BatchDeletionVector;
use super::{
    DiskFileEntry, IcebergSnapshotPayload, Snapshot, SnapshotTask,
    TableMetadata as MooncakeTableMetadata,
};
use crate::error::Result;
use crate::storage::cache::object_storage::base_cache::{
    CacheEntry as DataFileCacheEntry, CacheTrait, FileMetadata,
};
use crate::storage::cache::object_storage::object_storage_cache::ObjectStorageCache;
use crate::storage::compaction::table_compaction::{
    CompactedDataEntry, DataCompactionPayload, RemappedRecordLocation, SingleFileToCompact,
};
use crate::storage::iceberg::puffin_utils::PuffinBlobRef;
use crate::storage::iceberg::table_manager::TableManager;
use crate::storage::index::FileIndex;
use crate::storage::mooncake_table::shared_array::SharedRowBufferSnapshot;
use crate::storage::mooncake_table::snapshot_read_output::{
    DataFileForRead, ReadOutput as SnapshotReadOutput,
};
use crate::storage::mooncake_table::table_snapshot::{
    FileIndiceMergePayload, IcebergSnapshotDataCompactionPayload,
};
use crate::storage::mooncake_table::SnapshotOption;
use crate::storage::mooncake_table::{
    IcebergSnapshotImportPayload, IcebergSnapshotIndexMergePayload, MoonlinkRow,
};
use crate::storage::storage_utils::{FileId, TableId, TableUniqueFileId};
use crate::storage::storage_utils::{
    MooncakeDataFile, MooncakeDataFileRef, ProcessedDeletionRecord, RawDeletionRecord,
    RecordLocation,
};
use crate::table_notify::TableNotify;
use more_asserts as ma;
use parquet::arrow::AsyncArrowWriter;
use parquet::basic::{Compression, Encoding};
use parquet::file::properties::WriterProperties;
use std::cmp::Ordering;
use std::collections::{BTreeMap, HashMap, HashSet};
use std::mem::take;
use std::sync::Arc;
use tokio::sync::mpsc::Sender;
pub(crate) struct SnapshotTableState {
    /// Mooncake table metadata.
    pub(super) mooncake_table_metadata: Arc<MooncakeTableMetadata>,

    /// Current snapshot
    pub(super) current_snapshot: Snapshot,

    /// In memory RecordBatches, maps from batch id to in-memory batch.
    batches: BTreeMap<u64, InMemoryBatch>,

    /// Latest rows
    rows: Option<SharedRowBufferSnapshot>,

    // UNDONE(BATCH_INSERT):
    // Track uncommitted disk files/ batches from big batch insert

    // There're three types of deletion records:
    // 1. Uncommitted deletion logs
    // 2. Committed and persisted deletion logs, which are reflected at `snapshot::disk_files` along with the corresponding data files
    // 3. Committed but not yet persisted deletion logs
    //
    // Type-3, committed but not yet persisted deletion logs.
    pub(crate) committed_deletion_log: Vec<ProcessedDeletionRecord>,
    // Type-1: uncommitted deletion logs.
    pub(crate) uncommitted_deletion_log: Vec<Option<ProcessedDeletionRecord>>,

    /// Last commit point
    last_commit: RecordLocation,

    /// Data file cache.
    pub(super) data_file_cache: ObjectStorageCache,

    /// Table notifier.
    table_notify: Option<Sender<TableNotify>>,

    /// ---- Items not persisted to iceberg snapshot ----
    ///
    /// Iceberg snapshot is created in an async style, which means it doesn't correspond 1-1 to mooncake snapshot, so we need to ensure idempotency for iceberg snapshot payload.
    /// The following fields record unpersisted content, which will be placed in iceberg payload everytime.
    unpersisted_iceberg_records: UnpersistedIcebergSnapshotRecords,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct PuffinDeletionBlobAtRead {
    /// Index of local data files.
    pub data_file_index: u32,
    /// Index of puffin filepaths.
    pub puffin_file_index: u32,
    pub start_offset: u32,
    pub blob_size: u32,
}

// New file indices generated by new write operations and index merge operations are stored separately because they belong to different flush LSN.
#[derive(Clone, Debug, Default)]
struct UnpersistedIcebergSnapshotRecords {
    /// ===========================================
    /// Records generated by new writes
    /// ===========================================
    ///
    /// Unpersisted data files, new data files are appended to the end.
    unpersisted_data_files: Vec<MooncakeDataFileRef>,
    /// Unpersisted file indices, new indices are appended to the end.
    unpersisted_file_indices: Vec<FileIndex>,
    /// ===========================================
    /// Records generated by index merge
    /// ===========================================
    ///
    /// Unpersisted old merged file indices, which should not appear in the later iceberg snapshots.
    merged_file_indices_to_remove: Vec<FileIndex>,
    /// Unpersisted new merged indices, which should be added to the later iceberg snapshots.
    ///
    /// TODO(hjiang): Consider using hash set for faster lookup.
    merged_file_indices_to_add: Vec<FileIndex>,
    /// ===========================================
    /// Records generated by data compaction
    /// ===========================================
    ///
    /// Unpersisted old compacted data files, which should not appear in the later iceberg snapshots.
    compacted_data_files_to_remove: Vec<MooncakeDataFileRef>,
    /// Unpersisted new compacted data files, which should be added in the later iceberg snapshots.
    compacted_data_files_to_add: Vec<MooncakeDataFileRef>,
    /// Unpersisted old compacted file indices, which should not appear in the later iceberg snapshots.
    compacted_file_indices_to_remove: Vec<FileIndex>,
    /// Unpersisted new compacted file indices, which should be added in the later iceberg snapshots.
    compacted_file_indices_to_add: Vec<FileIndex>,
}

pub(crate) struct MooncakeSnapshotOutput {
    /// Committed LSN for mooncake snapshot.
    pub(crate) commit_lsn: u64,
    /// Iceberg snapshot payload.
    pub(crate) iceberg_snapshot_payload: Option<IcebergSnapshotPayload>,
    /// Data compaction payload.
    pub(crate) data_compaction_payload: Option<DataCompactionPayload>,
    /// File indice merge payload.
    pub(crate) file_indices_merge_payload: Option<FileIndiceMergePayload>,
    /// Evicted local data cache files to delete.
    pub(crate) evicted_data_files_to_delete: Vec<String>,
}

impl SnapshotTableState {
    pub(super) async fn new(
        metadata: Arc<MooncakeTableMetadata>,
        data_file_cache: ObjectStorageCache,
        // TODO(hjiang): Used when recovery enabled.
        _iceberg_table_manager: &mut dyn TableManager,
    ) -> Result<Self> {
        let mut batches = BTreeMap::new();
        batches.insert(0, InMemoryBatch::new(metadata.config.batch_size));

        Ok(Self {
            mooncake_table_metadata: metadata.clone(),
            current_snapshot: Snapshot::new(metadata.clone()),
            batches,
            rows: None,
            last_commit: RecordLocation::MemoryBatch(0, 0),
            data_file_cache,
            table_notify: None,
            committed_deletion_log: Vec::new(),
            uncommitted_deletion_log: Vec::new(),
            unpersisted_iceberg_records: UnpersistedIcebergSnapshotRecords::default(),
        })
    }

    /// Register event completion notifier.
    /// Notice it should be registered only once, which could be used to notify multiple events.
    pub(crate) fn register_table_notify(&mut self, table_notify: Sender<TableNotify>) {
        assert!(self.table_notify.is_none());
        self.table_notify = Some(table_notify);
    }

    /// Aggregate committed deletion logs, which could be persisted into iceberg snapshot.
    /// Return a mapping from local data filepath to its batch deletion vector.
    ///
    /// Precondition: all disk files have been integrated into snapshot.
    fn aggregate_committed_deletion_logs(
        &self,
        flush_lsn: u64,
    ) -> HashMap<MooncakeDataFileRef, BatchDeletionVector> {
        let mut aggregated_deletion_logs = HashMap::new();
        for cur_deletion_log in self.committed_deletion_log.iter() {
            ma::assert_le!(
                cur_deletion_log.lsn,
                self.current_snapshot.snapshot_version,
                "Committed deletion log {:?} is later than current snapshot LSN {}",
                cur_deletion_log,
                self.current_snapshot.snapshot_version
            );
            if cur_deletion_log.lsn > flush_lsn {
                continue;
            }
            if let RecordLocation::DiskFile(file_id, row_idx) = &cur_deletion_log.pos {
                let batch_deletion_vector = self.current_snapshot.disk_files.get(file_id).unwrap();
                let max_rows = batch_deletion_vector.batch_deletion_vector.get_max_rows();

                let cur_data_file = self
                    .current_snapshot
                    .disk_files
                    .get_key_value(file_id)
                    .unwrap()
                    .0
                    .clone();

                let deletion_vector = aggregated_deletion_logs
                    .entry(cur_data_file)
                    .or_insert_with(|| BatchDeletionVector::new(max_rows));
                assert!(deletion_vector.delete_row(*row_idx));
            }
        }
        aggregated_deletion_logs
    }

    /// Prune committed deletion logs for the given persisted records.
    fn prune_committed_deletion_logs(&mut self, task: &SnapshotTask) {
        // No iceberg snapshot persisted between two mooncake snapshot.
        if task.iceberg_persisted_records.flush_lsn.is_none() {
            return;
        }

        // Keep two types of committed logs: (1) in-memory committed deletion logs; (2) commit point after flush LSN.
        // All on-disk committed deletion logs, which are <= iceberg snapshot flush LSN could be pruned.
        let mut new_committed_deletion_log = vec![];
        let flush_point_lsn = task.iceberg_persisted_records.flush_lsn.unwrap();

        let old_committed_deletion_logs = std::mem::take(&mut self.committed_deletion_log);
        for cur_deletion_log in old_committed_deletion_logs.into_iter() {
            assert!(
                cur_deletion_log.lsn <= self.current_snapshot.snapshot_version,
                "Committed deletion log {:?} is later than current snapshot LSN {}",
                cur_deletion_log,
                self.current_snapshot.snapshot_version
            );
            if cur_deletion_log.lsn > flush_point_lsn {
                new_committed_deletion_log.push(cur_deletion_log);
                continue;
            }
            if let RecordLocation::MemoryBatch(_, _) = &cur_deletion_log.pos {
                new_committed_deletion_log.push(cur_deletion_log);
            }
        }

        self.committed_deletion_log = new_committed_deletion_log;
    }

    /// Update current mooncake snapshot with persisted deletion vector.
    fn update_current_snapshot_with_iceberg_snapshot(
        &mut self,
        puffin_blob_ref: HashMap<FileId, PuffinBlobRef>,
    ) {
        for (file_id, puffin_blob_ref) in puffin_blob_ref.into_iter() {
            let entry = self.current_snapshot.disk_files.get_mut(&file_id).unwrap();
            entry.puffin_deletion_blob = Some(puffin_blob_ref);
        }
    }

    /// Update disk files in the current snapshot from local data files to remote ones, meanwile unpin write-through cache file from data file cache.
    /// Return affected file indices, which are caused by data file updates, and will be removed.
    async fn update_data_files_to_persisted(
        &mut self,
        task: &SnapshotTask,
    ) -> (
        HashSet<FileIndex>,
        Vec<String>, /*evicted files to delete*/
    ) {
        if task.iceberg_persisted_records.data_files.is_empty() {
            return (HashSet::new(), Vec::new());
        }

        // Aggregate evicted files to delete.
        let mut evicted_files_to_delete = vec![];

        // Update disk file from local write through cache to iceberg persisted remote path.
        let mut affected_file_indices =
            HashSet::with_capacity(task.iceberg_persisted_records.file_indices.len());
        let persisted_data_files = &task.iceberg_persisted_records.data_files;
        for cur_data_file in persisted_data_files.iter() {
            let mut disk_file_entry = self
                .current_snapshot
                .disk_files
                .remove(cur_data_file)
                .unwrap();
            let cur_evicted_files = disk_file_entry
                .cache_handle
                .as_mut()
                .unwrap()
                .unreference()
                .await;
            evicted_files_to_delete.extend(cur_evicted_files);
            disk_file_entry.cache_handle = None;

            // One file index corresponds to one or many data files, so it's possible to have duplicates.
            affected_file_indices.insert(disk_file_entry.file_indice.as_ref().unwrap().clone());
            // Use old disk entry for now, file indices will be updated to remote paths.
            self.current_snapshot
                .disk_files
                .insert(cur_data_file.clone(), disk_file_entry);
        }

        (affected_file_indices, evicted_files_to_delete)
    }

    /// Update file indices in the current snapshot from local data files to remote ones.
    ///
    /// # Arguments
    ///
    /// * affected_file_indices: file indices affected by data files update, used to identify which file indices to remove.
    fn update_file_indices_to_persisted(
        &mut self,
        new_file_indices: Vec<FileIndex>,
        affected_file_indices: HashSet<FileIndex>,
    ) {
        if new_file_indices.is_empty() && affected_file_indices.is_empty() {
            return;
        }

        // Update file indice from local write through cache to iceberg persisted remote path.
        // TODO(hjiang): For better update performance, we might need to use hash set instead vector to store file indices.
        let cur_file_indices = std::mem::take(&mut self.current_snapshot.indices.file_indices);
        ma::assert_le!(affected_file_indices.len(), cur_file_indices.len());
        let mut updated_file_indices = Vec::with_capacity(cur_file_indices.len());
        for cur_file_index in cur_file_indices.into_iter() {
            if !affected_file_indices.contains(&cur_file_index) {
                updated_file_indices.push(cur_file_index);
            }
        }
        updated_file_indices.extend(new_file_indices.clone());
        self.current_snapshot.indices.file_indices = updated_file_indices;

        // Update disk file map to the updated file indices.
        for cur_file_index in self.current_snapshot.indices.file_indices.iter() {
            let cur_data_files = &cur_file_index.files;
            for cur_data_file in cur_data_files {
                self.current_snapshot
                    .disk_files
                    .get_mut(cur_data_file)
                    .unwrap()
                    .file_indice = Some(cur_file_index.clone());
            }
        }
    }

    /// Before iceberg snapshot, mooncake snapshot records local write through cache in disk file (which is local filepath).
    /// After a successful iceberg snapshot, update current snapshot's disk files and file indices to reference to remote paths,
    /// also import local write through cache to globally managed data file cache, so they could be pinned and evicted when necessary.
    ///
    /// Return evicted data files to delete when unreference existing disk file entries.
    async fn update_local_path_to_persisted_by_imported(
        &mut self,
        task: &SnapshotTask,
    ) -> Vec<String> {
        // Handle imported new data files and file indices.
        let (affected_file_indices, evicted_files_to_delete) =
            self.update_data_files_to_persisted(task).await;
        self.update_file_indices_to_persisted(
            task.iceberg_persisted_records.file_indices.clone(),
            affected_file_indices,
        );

        evicted_files_to_delete
    }

    /// Update unpersisted data files from successful iceberg snapshot operation.
    fn prune_persisted_data_files(&mut self, persisted_new_data_files: Vec<MooncakeDataFileRef>) {
        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .unpersisted_data_files
                .len(),
            persisted_new_data_files.len()
        );
        self.unpersisted_iceberg_records
            .unpersisted_data_files
            .drain(0..persisted_new_data_files.len());
    }

    /// Update unpersisted file indices from successful iceberg snapshot operation.
    fn prune_persisted_file_indices(&mut self, persisted_new_file_indices: Vec<FileIndex>) {
        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .unpersisted_file_indices
                .len(),
            persisted_new_file_indices.len()
        );
        self.unpersisted_iceberg_records
            .unpersisted_file_indices
            .drain(0..persisted_new_file_indices.len());
    }

    /// Util function to decide whether to create iceberg snapshot by new data files.
    fn create_iceberg_snapshot_by_data_files(
        &self,
        new_data_files: &[Arc<MooncakeDataFile>],
        force_create: bool,
    ) -> bool {
        let data_file_snapshot_threshold = if !force_create {
            self.mooncake_table_metadata
                .config
                .iceberg_snapshot_new_data_file_count()
        } else {
            1
        };
        new_data_files.len() >= data_file_snapshot_threshold
    }
    /// Util function to decide whether to create iceberg snapshot by deletion vectors.
    fn create_iceberg_snapshot_by_committed_logs(&self, force_create: bool) -> bool {
        let deletion_record_snapshot_threshold = if !force_create {
            self.mooncake_table_metadata
                .config
                .iceberg_snapshot_new_committed_deletion_log()
        } else {
            1
        };
        self.committed_deletion_log.len() >= deletion_record_snapshot_threshold
    }
    /// TODO(hjiang): Decide when to create iceberg snapshot by index merge and data compaction.
    /// Util function to decide whether to create iceberg snapshot by index merge results.
    fn create_iceberg_snapshot_by_index_merge(&self, force_create: bool) -> bool {
        force_create
            && !self
                .unpersisted_iceberg_records
                .merged_file_indices_to_add
                .is_empty()
    }
    /// Util function to decide whether to create iceberg snapshot by data compaction results.
    fn create_iceberg_snapshot_by_data_compaction(&self, force_create: bool) -> bool {
        force_create
            && (!self
                .unpersisted_iceberg_records
                .compacted_data_files_to_add
                .is_empty()
                || !self
                    .unpersisted_iceberg_records
                    .compacted_data_files_to_remove
                    .is_empty())
    }

    /// Util function to decide whether and what to compact data files.
    /// To simplify states (aka, avoid data compaction already in iceberg with those not), only merge those already persisted.
    fn get_payload_to_compact(&self) -> Option<DataCompactionPayload> {
        // Fast-path: not enough data files to trigger compaction.
        let all_disk_files = &self.current_snapshot.disk_files;
        if all_disk_files.len()
            < self
                .mooncake_table_metadata
                .config
                .data_compaction_config
                .data_file_to_compact as usize
        {
            return None;
        }

        // To simplify state management, only compact data files which have been persisted into iceberg table.
        let unpersisted_data_files = self
            .unpersisted_iceberg_records
            .unpersisted_data_files
            .iter()
            .cloned()
            .collect::<HashSet<_>>();
        let mut tentative_data_files_to_compact = vec![];
        let mut tentative_file_indices_to_compact = vec![];

        // TODO(hjiang): We should be able to early exit, if left items are not enough to reach the compaction threshold.
        for (cur_data_file, disk_file_entry) in all_disk_files.iter() {
            // Doesn't compact those unpersisted files.
            if unpersisted_data_files.contains(cur_data_file) {
                continue;
            }

            // Skip compaction if the file size exceeds threshold, AND it has no persisted deletion vectors.
            if disk_file_entry.file_size
                >= self
                    .mooncake_table_metadata
                    .config
                    .data_compaction_config
                    .data_file_final_size as usize
                && disk_file_entry.batch_deletion_vector.is_empty()
            {
                continue;
            }

            // Tentatively decide data file to compact.
            let single_file_to_compact = SingleFileToCompact {
                file_id: TableUniqueFileId {
                    table_id: TableId(self.mooncake_table_metadata.id),
                    file_id: cur_data_file.file_id(),
                },
                filepath: cur_data_file.file_path().to_string(),
                deletion_vector: disk_file_entry.puffin_deletion_blob.clone(),
            };
            tentative_data_files_to_compact.push(single_file_to_compact);

            // Tentatively decide corresponding file indices to compact.
            let cur_file_index = all_disk_files
                .get(cur_data_file)
                .unwrap()
                .file_indice
                .as_ref()
                .unwrap()
                .clone();
            tentative_file_indices_to_compact.push(cur_file_index);
        }

        if tentative_data_files_to_compact.len()
            < self
                .mooncake_table_metadata
                .config
                .data_compaction_config
                .data_file_to_compact as usize
        {
            return None;
        }

        Some(DataCompactionPayload {
            data_file_cache: self.data_file_cache.clone(),
            disk_files: tentative_data_files_to_compact,
            file_indices: tentative_file_indices_to_compact,
        })
    }

    /// Util function to decide whether and what to merge index.
    /// To simplify states (aka, avoid merging file indices already in iceberg with those not), only merge those already persisted.
    fn get_file_indices_to_merge(&self) -> HashSet<FileIndex> {
        // Fast-path: not enough file indices to trigger index merge.
        let mut file_indices_to_merge = HashSet::new();
        let all_file_indices = &self.current_snapshot.indices.file_indices;
        if all_file_indices.len()
            < self
                .mooncake_table_metadata
                .config
                .file_index_config
                .file_indices_to_merge as usize
        {
            return file_indices_to_merge;
        }

        // To simplify state management, only compact data files which have been persisted into iceberg table.
        let unpersisted_file_indices = self
            .unpersisted_iceberg_records
            .unpersisted_file_indices
            .iter()
            .cloned()
            .collect::<HashSet<_>>();

        for cur_file_index in all_file_indices.iter() {
            // Don't merge unpersisted file indices.
            if unpersisted_file_indices.contains(cur_file_index) {
                continue;
            }

            if cur_file_index.get_index_blocks_size()
                >= self
                    .mooncake_table_metadata
                    .config
                    .file_index_config
                    .index_block_final_size
            {
                continue;
            }
            assert!(file_indices_to_merge.insert(cur_file_index.clone()));
        }
        // To avoid too many small IO operations, only attempt an index merge when accumulated small indices exceeds the threshold.
        if file_indices_to_merge.len()
            >= self
                .mooncake_table_metadata
                .config
                .file_index_config
                .file_indices_to_merge as usize
        {
            return file_indices_to_merge;
        }
        HashSet::new()
    }

    fn prune_persisted_merged_indices(
        &mut self,
        old_merged_file_indices: &[FileIndex],
        new_merged_file_indices: &[FileIndex],
    ) {
        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .merged_file_indices_to_remove
                .len(),
            old_merged_file_indices.len()
        );
        self.unpersisted_iceberg_records
            .merged_file_indices_to_remove
            .drain(0..old_merged_file_indices.len());

        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .merged_file_indices_to_add
                .len(),
            new_merged_file_indices.len()
        );
        self.unpersisted_iceberg_records
            .merged_file_indices_to_add
            .drain(0..new_merged_file_indices.len());
    }

    fn prune_persisted_compacted_data(&mut self, task: &SnapshotTask) {
        let persisted_compaction_res = &task.iceberg_persisted_records;
        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .compacted_data_files_to_add
                .len(),
            persisted_compaction_res.new_compacted_data_files.len()
        );
        self.unpersisted_iceberg_records
            .compacted_data_files_to_add
            .drain(0..persisted_compaction_res.new_compacted_data_files.len());

        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .compacted_data_files_to_remove
                .len(),
            persisted_compaction_res.old_compacted_data_files.len()
        );
        self.unpersisted_iceberg_records
            .compacted_data_files_to_remove
            .drain(0..persisted_compaction_res.old_compacted_data_files.len());

        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .compacted_file_indices_to_add
                .len(),
            persisted_compaction_res.new_compacted_file_indices.len()
        );
        self.unpersisted_iceberg_records
            .compacted_file_indices_to_add
            .drain(0..persisted_compaction_res.new_compacted_file_indices.len());

        ma::assert_ge!(
            self.unpersisted_iceberg_records
                .compacted_file_indices_to_remove
                .len(),
            persisted_compaction_res.old_compacted_file_indices.len()
        );
        self.unpersisted_iceberg_records
            .compacted_file_indices_to_remove
            .drain(0..persisted_compaction_res.old_compacted_file_indices.len());
    }

    // Update current snapshot's file indices by adding and removing a few.
    fn update_file_indices_to_mooncake_snapshot_impl(
        &mut self,
        mut old_file_indices: HashSet<FileIndex>,
        new_file_indices: Vec<FileIndex>,
    ) {
        if old_file_indices.is_empty() {
            assert!(new_file_indices.is_empty());
            return;
        }

        // Update disk files' corresponding file indices.
        for cur_file_index in old_file_indices.iter() {
            for cur_data_file in cur_file_index.files.iter() {
                // File indices change could be caused by data compaction, so it's possible the disk file doesn't exist in current snapshot any more.
                if let Some(disk_file_entry) =
                    self.current_snapshot.disk_files.get_mut(cur_data_file)
                {
                    disk_file_entry.file_indice = None;
                }
            }
        }
        // Precondition: if any, data files have already been updated to disk files, so all data files referenced by new file indices already exists.
        for cur_file_index in new_file_indices.iter() {
            for cur_data_file in cur_file_index.files.iter() {
                let data_file_entry = self
                    .current_snapshot
                    .disk_files
                    .get_mut(cur_data_file)
                    .unwrap();
                data_file_entry.file_indice = Some(cur_file_index.clone());
            }
        }

        // Update current snapshot's file indices.
        let file_indices = std::mem::take(&mut self.current_snapshot.indices.file_indices);
        ma::assert_le!(old_file_indices.len(), file_indices.len());
        let updated_file_indices_len =
            file_indices.len() - old_file_indices.len() + new_file_indices.len();
        let mut updated_file_indices = Vec::with_capacity(updated_file_indices_len);

        for cur_file_indice in file_indices.into_iter() {
            if old_file_indices.remove(&cur_file_indice) {
                continue;
            }
            updated_file_indices.push(cur_file_indice);
        }
        updated_file_indices.extend(new_file_indices);
        self.current_snapshot.indices.file_indices = updated_file_indices;

        // Check all file indices to remove comes from current file indices.
        assert!(old_file_indices.is_empty());
    }

    // Update current snapshot's data files by adding and removing a few.
    // Here new data files are all local data files, which will be uploaded to remote by importing into iceberg table.
    // Return evicted data files from the data file cache to delete.
    async fn update_data_files_to_mooncake_snapshot_impl(
        &mut self,
        old_data_files: HashSet<MooncakeDataFileRef>,
        new_data_files: Vec<(MooncakeDataFileRef, CompactedDataEntry)>,
        remapped_data_files_after_compaction: HashMap<RecordLocation, RemappedRecordLocation>,
    ) -> Vec<String> {
        if old_data_files.is_empty() {
            assert!(new_data_files.is_empty());
            assert!(remapped_data_files_after_compaction.is_empty());
            return vec![];
        }

        let mut evicted_data_files = vec![];

        // Process new data files to import.
        ma::assert_ge!(self.current_snapshot.disk_files.len(), old_data_files.len());
        for (cur_new_data_file, cur_entry) in new_data_files.iter() {
            ma::assert_gt!(cur_entry.file_size, 0);
            let unique_file_id = TableUniqueFileId {
                table_id: TableId(self.mooncake_table_metadata.id),
                file_id: cur_new_data_file.file_id(),
            };
            let cache_entry = DataFileCacheEntry {
                cache_filepath: cur_new_data_file.file_path().clone(),
                file_metadata: FileMetadata {
                    file_size: cur_entry.file_size as u64,
                },
            };
            let (cache_handle, cur_evicted_files) = self
                .data_file_cache
                .import_cache_entry(unique_file_id, cache_entry)
                .await;
            evicted_data_files.extend(cur_evicted_files);

            self.current_snapshot.disk_files.insert(
                cur_new_data_file.clone(),
                DiskFileEntry {
                    file_size: cur_entry.file_size,
                    cache_handle: Some(cache_handle),
                    // Current implementation ensures only one file index for all new compacted data files.
                    file_indice: None,
                    batch_deletion_vector: BatchDeletionVector::new(
                        /*max_rows=*/ cur_entry.num_rows,
                    ),
                    puffin_deletion_blob: None,
                },
            );
        }

        // Process old data files to remove.
        for cur_old_data_file in old_data_files.into_iter() {
            let old_entry = self.current_snapshot.disk_files.remove(&cur_old_data_file);
            assert!(old_entry.is_some());

            let unique_file_id = TableUniqueFileId {
                table_id: TableId(self.mooncake_table_metadata.id),
                file_id: cur_old_data_file.file_id(),
            };

            // If the old entry is pinned cache handle, unreference.
            let old_entry = old_entry.unwrap();
            if let Some(mut cache_handle) = old_entry.cache_handle {
                let cur_evicted_files = cache_handle.unreference().await;
                evicted_data_files.extend(cur_evicted_files);

                // The old entry is no longer needed for mooncake table, directly mark it deleted from cache, so we could reclaim the disk space back ASAP.
                let cur_evicted_files = self
                    .data_file_cache
                    .delete_cache_entry(unique_file_id)
                    .await;
                evicted_data_files.extend(cur_evicted_files);
            }
            // Even if there's no pinned cache handle within current snapshot (since it's persisted), still try to delete it from cache if exists.
            else {
                let cur_evicted_files = self
                    .data_file_cache
                    .try_delete_cache_entry(unique_file_id)
                    .await;
                evicted_data_files.extend(cur_evicted_files);
            }

            // If no deletion record for this file, directly remove it, no need to do remapping.
            if old_entry.batch_deletion_vector.is_empty() {
                continue;
            }

            // If there's deletion record, try remap to the new compacted data file.
            let deleted_rows = old_entry.batch_deletion_vector.collect_deleted_rows();
            for cur_deleted_row in deleted_rows {
                let old_record_location =
                    RecordLocation::DiskFile(cur_old_data_file.file_id(), cur_deleted_row as usize);
                let new_record_location =
                    remapped_data_files_after_compaction.get(&old_record_location);
                // Case-1: The old record still exists, need to remap.
                if let Some(new_record_location) = new_record_location {
                    let new_deletion_entry = self
                        .current_snapshot
                        .disk_files
                        .get_mut(&new_record_location.new_data_file)
                        .unwrap();
                    new_deletion_entry
                        .batch_deletion_vector
                        .delete_row(new_record_location.record_location.get_row_idx());
                }
                // Case-2: The old record has already been compacted, directly skip.
            }
        }

        evicted_data_files
    }

    fn update_file_indices_merge_to_mooncake_snapshot(
        &mut self,
        old_merged_file_indices: HashSet<FileIndex>,
        new_merged_file_indices: Vec<FileIndex>,
    ) {
        self.update_file_indices_to_mooncake_snapshot_impl(
            old_merged_file_indices,
            new_merged_file_indices,
        );
    }

    /// Reflect data compaction results to mooncake snapshot.
    /// Return evicted data files to delete due to data compaction.
    async fn update_data_compaction_to_mooncake_snapshot(
        &mut self,
        task: &SnapshotTask,
    ) -> Vec<String> {
        if task.data_compaction_result.is_empty() {
            return vec![];
        }

        // NOTICE: Update data files before file indices, so when update file indices, data files for new file indices already exist in disk files map.
        let data_compaction_res = task.data_compaction_result.clone();
        let mut evicted_data_files = self
            .update_data_files_to_mooncake_snapshot_impl(
                data_compaction_res.old_data_files,
                data_compaction_res.new_data_files,
                data_compaction_res.remapped_data_files,
            )
            .await;
        self.update_file_indices_to_mooncake_snapshot_impl(
            data_compaction_res.old_file_indices,
            data_compaction_res.new_file_indices,
        );

        // Apply evicted data files to delete within data compaction process.
        evicted_data_files.extend(
            task.data_compaction_result
                .evicted_files_to_delete
                .iter()
                .cloned()
                .to_owned(),
        );

        evicted_data_files
    }

    fn buffer_unpersisted_iceberg_new_data_files(&mut self, task: &SnapshotTask) {
        let new_data_files = task.get_new_data_files();
        self.unpersisted_iceberg_records
            .unpersisted_data_files
            .extend(new_data_files);
    }
    fn buffer_unpersisted_iceberg_new_file_indices(&mut self, task: &SnapshotTask) {
        let new_file_indices = task.get_new_file_indices();
        self.unpersisted_iceberg_records
            .unpersisted_file_indices
            .extend(new_file_indices);
    }
    fn buffer_unpersisted_iceberg_merged_file_indices(&mut self, task: &SnapshotTask) {
        self.unpersisted_iceberg_records
            .merged_file_indices_to_add
            .extend(task.new_merged_file_indices.to_owned());
        self.unpersisted_iceberg_records
            .merged_file_indices_to_remove
            .extend(task.old_merged_file_indices.to_owned());
    }

    fn buffer_unpersisted_iceberg_compaction_data(&mut self, task: &SnapshotTask) {
        let data_compaction_res = &task.data_compaction_result;
        if data_compaction_res.is_empty() {
            return;
        }

        let data_compaction_res = data_compaction_res.clone();
        let l = data_compaction_res.new_data_files.len();

        let mut new_compacted_data_files = Vec::with_capacity(l);
        for (new_data_file, _) in data_compaction_res.new_data_files.into_iter() {
            new_compacted_data_files.push(new_data_file.clone());
        }

        self.unpersisted_iceberg_records
            .compacted_data_files_to_add
            .extend(new_compacted_data_files);
        self.unpersisted_iceberg_records
            .compacted_data_files_to_remove
            .extend(data_compaction_res.old_data_files);
        self.unpersisted_iceberg_records
            .compacted_file_indices_to_add
            .extend(data_compaction_res.new_file_indices);
        self.unpersisted_iceberg_records
            .compacted_file_indices_to_remove
            .extend(data_compaction_res.old_file_indices);
    }

    /// Remap single record location after compaction.
    /// Return if remap succeeds.
    fn remap_record_location_after_compaction(
        deletion_log: &mut ProcessedDeletionRecord,
        task: &mut SnapshotTask,
    ) -> bool {
        if task.data_compaction_result.is_empty() {
            return false;
        }

        let old_record_location = &deletion_log.pos;
        let remapped_data_files_after_compaction = &mut task.data_compaction_result;
        let new_record_location = remapped_data_files_after_compaction
            .remapped_data_files
            .remove(old_record_location);
        if new_record_location.is_none() {
            return false;
        }
        deletion_log.pos = new_record_location.unwrap().record_location;
        true
    }

    /// Data compaction might delete existing persisted files, which invalidates record locations and requires a record location remap.
    fn remap_and_prune_deletion_logs_after_compaction(&mut self, task: &mut SnapshotTask) {
        // No need to prune and remap if no compaction happening.
        if task.data_compaction_result.is_empty() {
            return;
        }

        // Remap and prune committed deletion log.
        let mut new_committed_deletion_log = vec![];
        let old_committed_deletion_log = std::mem::take(&mut self.committed_deletion_log);
        for mut cur_deletion_log in old_committed_deletion_log.into_iter() {
            if let Some(file_id) = cur_deletion_log.get_file_id() {
                // Case-1: the deletion log doesn't indicate a compacted data file.
                if !task
                    .data_compaction_result
                    .old_data_files
                    .contains(&file_id)
                {
                    new_committed_deletion_log.push(cur_deletion_log);
                    continue;
                }
                // Case-2: the deletion log exists in the compacted new data file, perform a remap.
                let remap_succ =
                    Self::remap_record_location_after_compaction(&mut cur_deletion_log, task);
                if remap_succ {
                    new_committed_deletion_log.push(cur_deletion_log);
                    continue;
                }
                // Case-3: the deletion log doesn't exist in the compacted new file, directly remove it.
            } else {
                new_committed_deletion_log.push(cur_deletion_log);
            }
        }
        self.committed_deletion_log = new_committed_deletion_log;

        // Remap uncommitted deletion log.
        for cur_deletion_log in &mut self.uncommitted_deletion_log {
            if cur_deletion_log.is_none() {
                continue;
            }
            Self::remap_record_location_after_compaction(cur_deletion_log.as_mut().unwrap(), task);
        }
    }

    fn get_iceberg_snapshot_payload(
        &self,
        flush_lsn: u64,
        new_committed_deletion_logs: HashMap<MooncakeDataFileRef, BatchDeletionVector>,
    ) -> IcebergSnapshotPayload {
        IcebergSnapshotPayload {
            flush_lsn,
            import_payload: IcebergSnapshotImportPayload {
                data_files: self
                    .unpersisted_iceberg_records
                    .unpersisted_data_files
                    .to_vec(),
                new_deletion_vector: new_committed_deletion_logs,
                file_indices: self
                    .unpersisted_iceberg_records
                    .unpersisted_file_indices
                    .to_vec(),
            },
            index_merge_payload: IcebergSnapshotIndexMergePayload {
                new_file_indices_to_import: self
                    .unpersisted_iceberg_records
                    .merged_file_indices_to_add
                    .to_vec(),
                old_file_indices_to_remove: self
                    .unpersisted_iceberg_records
                    .merged_file_indices_to_remove
                    .to_vec(),
            },
            data_compaction_payload: IcebergSnapshotDataCompactionPayload {
                new_data_files_to_import: self
                    .unpersisted_iceberg_records
                    .compacted_data_files_to_add
                    .to_vec(),
                old_data_files_to_remove: self
                    .unpersisted_iceberg_records
                    .compacted_data_files_to_remove
                    .to_vec(),
                new_file_indices_to_import: self
                    .unpersisted_iceberg_records
                    .compacted_file_indices_to_add
                    .to_vec(),
                old_file_indices_to_remove: self
                    .unpersisted_iceberg_records
                    .compacted_file_indices_to_remove
                    .to_vec(),
            },
        }
    }

    /// Unreference pinned cache handles used in read operations.
    /// Return evicted data files to delete.
    async fn unreference_read_cache_handles(&mut self, task: &mut SnapshotTask) -> Vec<String> {
        // Aggregate evicted data files to delete.
        let mut evicted_files_to_delete = vec![];

        for cur_cache_handle in task.read_cache_handles.iter_mut() {
            let cur_evicted_files = cur_cache_handle.unreference().await;
            evicted_files_to_delete.extend(cur_evicted_files);
        }

        evicted_files_to_delete
    }

    /// Take read request result and update mooncake snapshot.
    /// Return evicted data files to delete.
    async fn update_snapshot_by_read_request_results(
        &mut self,
        task: &mut SnapshotTask,
    ) -> Vec<String> {
        // Unpin cached files used in the read request.
        self.unreference_read_cache_handles(task).await
    }

    /// Unreference all pinned data files.
    /// Return all evicted files to evict
    pub(crate) async fn unreference_all_cache_handles(&mut self) -> Vec<String> {
        // Aggregate evicted files to delete.
        let mut evicted_files_to_delete = vec![];

        for (_, disk_file_entry) in self.current_snapshot.disk_files.iter_mut() {
            let cache_handle = &mut disk_file_entry.cache_handle;
            if let Some(cache_handle) = cache_handle {
                let cur_evicted_files = cache_handle.unreference().await;
                evicted_files_to_delete.extend(cur_evicted_files);
            }
        }

        evicted_files_to_delete
    }

    pub(super) async fn update_snapshot(
        &mut self,
        mut task: SnapshotTask,
        opt: SnapshotOption,
    ) -> MooncakeSnapshotOutput {
        // All evicted data files by the data file cache.
        let mut evicted_data_files_to_delete = vec![];

        // Reflect read request result to mooncake snapshot.
        let completed_read_evicted_data_files = self
            .update_snapshot_by_read_request_results(&mut task)
            .await;
        evicted_data_files_to_delete.extend(completed_read_evicted_data_files);

        // Reflect iceberg snapshot to mooncake snapshot.
        //
        // There're a few things to do:
        // 1. Prune unpersisted fields.
        // 2. Update persisted data files and index blocks from local path to remote path, and import into cache.
        // 3. Update mooncake snapshot with index merge and data compaction results.
        //
        // Update data files and file indices' local file path to remote file path, it only applies to newly imported data files and file indices,
        // because both index merge and data compaction only apply to those already persisted into iceberg table.
        let persistence_evicted_data_files =
            self.update_local_path_to_persisted_by_imported(&task).await;
        evicted_data_files_to_delete.extend(persistence_evicted_data_files);

        self.update_current_snapshot_with_iceberg_snapshot(std::mem::take(
            &mut task.iceberg_persisted_records.puffin_blob,
        ));
        // Update disk files' disk entries and file indices from merged indices.
        self.update_file_indices_merge_to_mooncake_snapshot(
            task.old_merged_file_indices.clone(),
            task.new_merged_file_indices.clone(),
        );
        // Update disk file's disk entries and file indices from compacted data files and file indices.
        // Also remap committed deletion logs if applicable.
        let compaction_evicted_data_files = self
            .update_data_compaction_to_mooncake_snapshot(&task)
            .await;
        evicted_data_files_to_delete.extend(compaction_evicted_data_files);

        // Prune unpersisted records.
        self.prune_committed_deletion_logs(&task);
        self.prune_persisted_data_files(std::mem::take(
            &mut task.iceberg_persisted_records.data_files,
        ));
        self.prune_persisted_file_indices(std::mem::take(
            &mut task.iceberg_persisted_records.file_indices,
        ));
        self.prune_persisted_merged_indices(
            &task.iceberg_persisted_records.old_merged_file_indices,
            &task.iceberg_persisted_records.new_merged_file_indices,
        );
        self.prune_persisted_compacted_data(&task);

        // Sync buffer snapshot states into unpersisted iceberg content.
        self.buffer_unpersisted_iceberg_new_data_files(&task);
        self.buffer_unpersisted_iceberg_new_file_indices(&task);
        self.buffer_unpersisted_iceberg_merged_file_indices(&task);
        self.buffer_unpersisted_iceberg_compaction_data(&task);

        // Apply buffered change to current mooncake snapshot.
        let stream_evicted_cache_files = self.apply_transaction_stream(&mut task).await;
        self.merge_mem_indices(&mut task);
        self.finalize_batches(&mut task);
        let batch_evicted_cache_files = self.integrate_disk_slices(&mut task).await;
        evicted_data_files_to_delete.extend(stream_evicted_cache_files);
        evicted_data_files_to_delete.extend(batch_evicted_cache_files);

        // Apply data compaction to committed deletion logs.
        self.remap_and_prune_deletion_logs_after_compaction(&mut task);

        // After data compaction and index merge changes have been applied to snapshot, processed deletion record will point to the new record location.
        self.rows = take(&mut task.new_rows);
        self.process_deletion_log(&mut task).await;

        if let Some(flush_lsn) = task.new_flush_lsn {
            self.current_snapshot.data_file_flush_lsn = Some(flush_lsn);
        }
        if task.new_commit_lsn != 0 {
            self.current_snapshot.snapshot_version = task.new_commit_lsn;
        }
        if let Some(cp) = task.new_commit_point {
            self.last_commit = cp;
        }

        // Till this point, committed changes have been reflected to current snapshot; sync the latest change to iceberg.
        // To reduce iceberg persistence overhead, we only snapshot when (1) there're persisted data files, or (2) accumulated unflushed deletion vector exceeds threshold.
        let mut iceberg_snapshot_payload: Option<IcebergSnapshotPayload> = None;
        let flush_by_data_files = self.create_iceberg_snapshot_by_data_files(
            self.unpersisted_iceberg_records
                .unpersisted_data_files
                .as_slice(),
            opt.force_create,
        );
        let flush_by_deletion_logs =
            self.create_iceberg_snapshot_by_committed_logs(opt.force_create);
        let flush_by_merge_file_indices =
            self.create_iceberg_snapshot_by_index_merge(opt.force_create);
        let flush_by_data_compaction =
            self.create_iceberg_snapshot_by_data_compaction(opt.force_create);

        // Decide whether to perform a data compaction.
        let mut data_compaction_payload: Option<DataCompactionPayload> = None;
        if !opt.skip_data_file_compaction {
            data_compaction_payload = self.get_payload_to_compact();
        }

        // Decide whether to merge an index merge, which cannot be performed together with data compaction.
        let mut file_indices_merge_payload: Option<FileIndiceMergePayload> = None;
        if !opt.skip_file_indices_merge && data_compaction_payload.is_none() {
            let file_indices_to_merge = self.get_file_indices_to_merge();
            if !file_indices_to_merge.is_empty() {
                file_indices_merge_payload = Some(FileIndiceMergePayload {
                    file_indices: file_indices_to_merge,
                });
            }
        }

        // TODO(hjiang): Add whether to flush based on merged file indices.
        if !opt.skip_iceberg_snapshot
            && self.current_snapshot.data_file_flush_lsn.is_some()
            && (flush_by_data_files
                || flush_by_deletion_logs
                || flush_by_merge_file_indices
                || flush_by_data_compaction)
        {
            // Getting persistable committed deletion logs is not cheap, which requires iterating through all logs,
            // so we only aggregate when there's committed deletion.
            let flush_lsn = self.current_snapshot.data_file_flush_lsn.unwrap();
            let aggregated_committed_deletion_logs =
                self.aggregate_committed_deletion_logs(flush_lsn);

            // Only create iceberg snapshot when there's something to import.
            if !aggregated_committed_deletion_logs.is_empty()
                || flush_by_data_files
                || flush_by_merge_file_indices
                || flush_by_data_compaction
            {
                iceberg_snapshot_payload =
                    Some(self.get_iceberg_snapshot_payload(
                        flush_lsn,
                        aggregated_committed_deletion_logs,
                    ));
            }
        }

        // Expensive assertion, which is only enabled in unit tests.
        #[cfg(test)]
        {
            self.assert_current_snapshot_consistent();
        }

        MooncakeSnapshotOutput {
            commit_lsn: self.current_snapshot.snapshot_version,
            iceberg_snapshot_payload,
            data_compaction_payload,
            file_indices_merge_payload,
            evicted_data_files_to_delete,
        }
    }

    /// Test util function to assert the mapping between data files and file indices are consistent.
    #[cfg(test)]
    fn assert_data_files_and_file_indices_consistent(&self) {
        // (1) Get data file to file indices mapping from [`disk_files`].
        let mut data_file_to_file_indices_1 =
            HashMap::with_capacity(self.current_snapshot.disk_files.len());
        for (cur_disk_file, cur_disk_file_entry) in self.current_snapshot.disk_files.iter() {
            let cur_file_index = cur_disk_file_entry.file_indice.as_ref().unwrap().clone();
            data_file_to_file_indices_1.insert(cur_disk_file.clone(), cur_file_index);
        }
        // (2) Get data file to file indices mapping from [`file_indices`].
        let mut data_file_to_file_indices_2 =
            HashMap::with_capacity(self.current_snapshot.disk_files.len());
        let file_indices = &self.current_snapshot.indices.file_indices;
        for cur_file_index in file_indices {
            for cur_data_file in cur_file_index.files.iter() {
                let old_entry = data_file_to_file_indices_2
                    .insert(cur_data_file.clone(), cur_file_index.clone());
                assert!(old_entry.is_none());
            }
        }
        // Assert mapping inferred from disk files and file indices are the same.
        assert_eq!(data_file_to_file_indices_1, data_file_to_file_indices_2);
    }

    /// Test util functions to assert current snapshot is at a consistent state.
    #[cfg(test)]
    fn assert_current_snapshot_consistent(&self) {
        // Check file indices and disk files are consistent.
        self.assert_data_files_and_file_indices_consistent();
    }

    fn merge_mem_indices(&mut self, task: &mut SnapshotTask) {
        for idx in take(&mut task.new_mem_indices) {
            self.current_snapshot.indices.insert_memory_index(idx);
        }
    }

    fn finalize_batches(&mut self, task: &mut SnapshotTask) {
        if task.new_record_batches.is_empty() {
            return;
        }

        let incoming = take(&mut task.new_record_batches);
        // close previouslyopen batch
        assert!(self.batches.values().last().unwrap().data.is_none());
        self.batches.last_entry().unwrap().get_mut().data = Some(incoming[0].1.clone());

        // start a fresh empty batch after the newest data
        let batch_size = self.current_snapshot.metadata.config.batch_size;
        let next_id = incoming.last().unwrap().0 + 1;
        self.batches.insert(next_id, InMemoryBatch::new(batch_size));

        // add completed batches
        self.batches
            .extend(incoming.into_iter().skip(1).map(|(id, rb)| {
                (
                    id,
                    InMemoryBatch {
                        data: Some(rb.clone()),
                        deletions: BatchDeletionVector::new(rb.num_rows()),
                    },
                )
            }));
    }

    /// Return files evicted from data file cache.
    async fn integrate_disk_slices(&mut self, task: &mut SnapshotTask) -> Vec<String> {
        // Aggregate evicted data cache files to delete.
        let mut evicted_files = vec![];

        for mut slice in take(&mut task.new_disk_slices) {
            let write_lsn = slice.lsn();
            let lsn = write_lsn.expect("commited datafile should have a valid LSN");

            // Register new files into mooncake snapshot, add it into cache, and record LSN map.
            for (file, file_attrs) in slice.output_files().iter() {
                ma::assert_gt!(file_attrs.file_size, 0);
                task.disk_file_lsn_map.insert(file.file_id(), lsn);
                let unique_file_id = TableUniqueFileId {
                    table_id: TableId(self.mooncake_table_metadata.id),
                    file_id: file.file_id(),
                };
                let (cache_handle, cur_evicted_files) = self
                    .data_file_cache
                    .import_cache_entry(
                        unique_file_id,
                        DataFileCacheEntry {
                            cache_filepath: file.file_path().clone(),
                            file_metadata: FileMetadata {
                                file_size: file_attrs.file_size as u64,
                            },
                        },
                    )
                    .await;
                evicted_files.extend(cur_evicted_files);
                self.current_snapshot.disk_files.insert(
                    file.clone(),
                    DiskFileEntry {
                        file_size: file_attrs.file_size,
                        cache_handle: Some(cache_handle),
                        file_indice: Some(slice.get_file_indice().as_ref().unwrap().clone()),
                        batch_deletion_vector: BatchDeletionVector::new(file_attrs.row_num),
                        puffin_deletion_blob: None,
                    },
                );
            }

            // remap deletions written *after* this slices LSN
            let cut = self.committed_deletion_log.partition_point(|d| {
                d.lsn
                    <= write_lsn.expect(
                        "Critical: LSN is None after it should have been updated by commit process",
                    )
            });

            self.committed_deletion_log[cut..]
                .iter_mut()
                .for_each(|d| slice.remap_deletion_if_needed(d));

            self.uncommitted_deletion_log
                .iter_mut()
                .flatten()
                .for_each(|d| slice.remap_deletion_if_needed(d));

            // swap indices and drop in-memory batches that were flushed
            if let Some(on_disk_index) = slice.take_index() {
                self.current_snapshot
                    .indices
                    .insert_file_index(on_disk_index);
            }
            self.current_snapshot
                .indices
                .delete_memory_index(slice.old_index());

            slice.input_batches().iter().for_each(|b| {
                self.batches.remove(&b.id);
            });
        }

        evicted_files
    }

    async fn match_deletions_with_identical_key_and_lsn(
        &self,
        deletions: &[RawDeletionRecord],
        index_lookup_result: Vec<RecordLocation>,
        file_id_to_lsn: &HashMap<FileId, u64>,
    ) -> Vec<ProcessedDeletionRecord> {
        let mut candidates: Vec<RecordLocation> = index_lookup_result
            .into_iter()
            .filter(|loc| {
                !self.is_deleted(loc)
                    && Self::is_visible(loc, file_id_to_lsn, deletions.first().unwrap().lsn)
            })
            .collect();
        // This optimization is important when working with table without primary key.
        // Postgres never distinguish row with same value, so they will almost always be processed together.
        // Thus we can avoid full row identity comparison if we also process them together.
        match candidates.len().cmp(&deletions.len()) {
            Ordering::Equal => candidates
                .into_iter()
                .zip(deletions.iter())
                .map(|(loc, deletion)| Self::build_processed_deletion(deletion, loc))
                .collect(),
            Ordering::Less => panic!(
                "find less than expected candidates to deletions {:?}",
                deletions
            ),
            Ordering::Greater => {
                let mut processed_deletions = Vec::new();
                // multiple candidates  disambiguate via full row identity comparison.
                for deletion in deletions.iter() {
                    let identity = deletion
                        .row_identity
                        .as_ref()
                        .expect("row_identity required when multiple matches");
                    let mut target_position: Option<RecordLocation> = None;
                    for (idx, loc) in candidates.iter().enumerate() {
                        let matches = self.matches_identity(loc, identity).await;
                        if matches {
                            target_position = Some(candidates.swap_remove(idx));
                            break;
                        }
                    }
                    processed_deletions.push(Self::build_processed_deletion(
                        deletion,
                        target_position.unwrap(),
                    ));
                }
                processed_deletions
            }
        }
    }

    #[inline]
    fn build_processed_deletion(
        deletion: &RawDeletionRecord,
        pos: RecordLocation,
    ) -> ProcessedDeletionRecord {
        ProcessedDeletionRecord {
            pos,
            lsn: deletion.lsn,
        }
    }

    /// Returns `true` if the location has already been marked deleted.
    fn is_deleted(&self, loc: &RecordLocation) -> bool {
        match loc {
            RecordLocation::MemoryBatch(batch_id, row_id) => self
                .batches
                .get(batch_id)
                .expect("missing batch")
                .deletions
                .is_deleted(*row_id),

            RecordLocation::DiskFile(file_id, row_id) => self
                .current_snapshot
                .disk_files
                .get(file_id)
                .expect("missing disk file")
                .batch_deletion_vector
                .is_deleted(*row_id),
        }
    }

    fn is_visible(loc: &RecordLocation, file_id_to_lsn: &HashMap<FileId, u64>, lsn: u64) -> bool {
        match loc {
            RecordLocation::MemoryBatch(_, _) => true,
            RecordLocation::DiskFile(file_id, _) => {
                file_id_to_lsn.get(file_id).is_none() || file_id_to_lsn.get(file_id).unwrap() < &lsn
            }
        }
    }

    /// Verifies that `loc` matches the provided `identity`.
    async fn matches_identity(&self, loc: &RecordLocation, identity: &MoonlinkRow) -> bool {
        match loc {
            RecordLocation::MemoryBatch(batch_id, row_id) => {
                let batch = self.batches.get(batch_id).expect("missing batch");
                identity.equals_record_batch_at_offset(
                    batch.data.as_ref().expect("batch missing data"),
                    *row_id,
                    &self.current_snapshot.metadata.identity,
                )
            }
            RecordLocation::DiskFile(file_id, row_id) => {
                let (file, _) = self
                    .current_snapshot
                    .disk_files
                    .get_key_value(file_id)
                    .expect("missing disk file");
                identity
                    .equals_parquet_at_offset(
                        file.file_path(),
                        *row_id,
                        &self.current_snapshot.metadata.identity,
                    )
                    .await
            }
        }
    }

    /// Commit a row deletion record.
    fn commit_deletion(&mut self, deletion: ProcessedDeletionRecord) {
        match &deletion.pos {
            RecordLocation::MemoryBatch(batch_id, row_id) => {
                if self.batches.contains_key(batch_id) {
                    // Possible we deleted an in memory row that was flushed
                    let res = self
                        .batches
                        .get_mut(batch_id)
                        .unwrap()
                        .deletions
                        .delete_row(*row_id);
                    assert!(res);
                }
            }
            RecordLocation::DiskFile(file_name, row_id) => {
                let res = self
                    .current_snapshot
                    .disk_files
                    .get_mut(file_name)
                    .unwrap()
                    .batch_deletion_vector
                    .delete_row(*row_id);
                assert!(res);
            }
        }
        self.committed_deletion_log.push(deletion);
    }

    async fn process_deletion_log(&mut self, task: &mut SnapshotTask) {
        self.advance_pending_deletions(task);
        self.apply_new_deletions(task).await;
    }

    /// Update, commit, or re-queue previously seen deletions.
    fn advance_pending_deletions(&mut self, task: &SnapshotTask) {
        let mut still_uncommitted = Vec::new();

        for mut entry in take(&mut self.uncommitted_deletion_log) {
            let deletion = entry.take().unwrap();
            if deletion.lsn <= task.new_commit_lsn {
                self.commit_deletion(deletion);
            } else {
                still_uncommitted.push(Some(deletion));
            }
        }

        self.uncommitted_deletion_log = still_uncommitted;
    }

    fn add_processed_deletion(
        &mut self,
        deletions: Vec<ProcessedDeletionRecord>,
        new_commit_lsn: u64,
    ) {
        for deletion in deletions.into_iter() {
            if deletion.lsn <= new_commit_lsn {
                self.commit_deletion(deletion);
            } else {
                self.uncommitted_deletion_log.push(Some(deletion));
            }
        }
    }

    /// Convert raw deletions discovered by the snapshot task and either commit
    /// them or defer until their LSN becomes visible.
    async fn apply_new_deletions(&mut self, task: &mut SnapshotTask) {
        let mut new_deletions = take(&mut task.new_deletions);
        let mut already_processed = Vec::new();
        new_deletions.retain(|deletion| {
            if let Some(pos) = deletion.pos {
                already_processed.push(Self::build_processed_deletion(deletion, pos.into()));
                false
            } else {
                true
            }
        });
        self.add_processed_deletion(already_processed, task.new_commit_lsn);
        new_deletions.sort_by_key(|deletion| deletion.lookup_key);
        if new_deletions.is_empty() {
            return;
        }
        let mut index_lookup_result = self
            .current_snapshot
            .indices
            .find_records(&new_deletions)
            .await;
        index_lookup_result.sort_by_key(|(key, _)| *key);
        let mut i = 0;
        let mut j = 0;
        while i < new_deletions.len() {
            let start_i = i;
            while i < new_deletions.len()
                && new_deletions[i].lookup_key == new_deletions[start_i].lookup_key
                && new_deletions[i].lsn == new_deletions[start_i].lsn
            {
                i += 1;
            }
            let deletions = &new_deletions[start_i..i];
            let mut lookup_result = Vec::new();
            while index_lookup_result[j].0 != new_deletions[start_i].lookup_key {
                j += 1;
            }
            let mut j_end = j;
            while j_end < index_lookup_result.len()
                && index_lookup_result[j_end].0 == new_deletions[start_i].lookup_key
            {
                lookup_result.push(index_lookup_result[j_end].1.clone());
                j_end += 1;
            }
            let processed_deletions = self
                .match_deletions_with_identical_key_and_lsn(
                    deletions,
                    lookup_result,
                    &task.disk_file_lsn_map,
                )
                .await;
            self.add_processed_deletion(processed_deletions, task.new_commit_lsn);
        }
    }

    /// Get committed deletion record for current snapshot.
    fn get_deletion_records(
        &self,
    ) -> (
        Vec<String>,                   /*puffin filepaths*/
        Vec<PuffinDeletionBlobAtRead>, /*deletion vector puffin*/
        Vec<(
            u32, /*index of disk file in snapshot*/
            u32, /*row id*/
        )>,
    ) {
        // Get puffin blobs for deletion vector.
        let mut puffin_filepaths = vec![];
        let mut deletion_vector_blob_at_read = vec![];
        for (idx, (_, disk_deletion_vector)) in self.current_snapshot.disk_files.iter().enumerate()
        {
            if disk_deletion_vector.puffin_deletion_blob.is_none() {
                continue;
            }
            let puffin_deletion_blob = disk_deletion_vector.puffin_deletion_blob.as_ref().unwrap();
            puffin_filepaths.push(puffin_deletion_blob.puffin_filepath.clone());
            let puffin_file_index = puffin_filepaths.len() - 1;
            deletion_vector_blob_at_read.push(PuffinDeletionBlobAtRead {
                data_file_index: idx as u32,
                puffin_file_index: puffin_file_index as u32,
                start_offset: puffin_deletion_blob.start_offset,
                blob_size: puffin_deletion_blob.blob_size,
            });
        }

        // Get committed but un-persisted deletion vector.
        let mut ret = Vec::new();
        for deletion in self.committed_deletion_log.iter() {
            if let RecordLocation::DiskFile(file_id, row_id) = &deletion.pos {
                for (id, (file, _)) in self.current_snapshot.disk_files.iter().enumerate() {
                    if file.file_id() == *file_id {
                        ret.push((id as u32, *row_id as u32));
                        break;
                    }
                }
            }
        }
        (puffin_filepaths, deletion_vector_blob_at_read, ret)
    }

    /// Util function to get read state, which returns all current data files information.
    /// If a data file already has a pinned reference, increment the reference count directly to avoid unnecessary IO.
    async fn get_read_files_for_read(&mut self) -> Vec<DataFileForRead> {
        let mut data_files_for_read = Vec::with_capacity(self.current_snapshot.disk_files.len());

        for (file, _) in self.current_snapshot.disk_files.iter() {
            let file_id = TableUniqueFileId {
                table_id: TableId(self.mooncake_table_metadata.id),
                file_id: file.file_id(),
            };
            data_files_for_read.push(DataFileForRead::RemoteFilePath((
                file_id,
                file.file_path().to_string(),
            )));
        }

        data_files_for_read
    }

    pub(crate) async fn request_read(&mut self) -> Result<SnapshotReadOutput> {
        let mut data_file_paths = self.get_read_files_for_read().await;
        let mut associated_files = Vec::new();
        let (puffin_file_paths, deletion_vectors_at_read, position_deletes) =
            self.get_deletion_records();

        // For committed but not persisted records, we create a temporary file for them, which gets deleted after query completion.
        let file_path = self.current_snapshot.get_name_for_inmemory_file();
        let filepath_exists = tokio::fs::try_exists(&file_path).await?;
        if filepath_exists {
            data_file_paths.push(DataFileForRead::TemporaryDataFile(
                file_path.to_string_lossy().to_string(),
            ));
            associated_files.push(file_path.to_string_lossy().to_string());
            return Ok(SnapshotReadOutput {
                data_file_paths,
                puffin_file_paths,
                deletion_vectors: deletion_vectors_at_read,
                position_deletes,
                associated_files,
                data_file_cache: Some(self.data_file_cache.clone()),
                table_notifier: Some(self.table_notify.as_ref().unwrap().clone()),
            });
        }

        assert!(matches!(
            self.last_commit,
            RecordLocation::MemoryBatch(_, _)
        ));
        let (batch_id, row_id) = self.last_commit.clone().into();
        if batch_id > 0 || row_id > 0 {
            // add all batches
            let mut filtered_batches = Vec::new();
            let schema = self.current_snapshot.metadata.schema.clone();
            for (id, batch) in self.batches.iter() {
                if *id < batch_id {
                    if let Some(filtered_batch) = batch.get_filtered_batch()? {
                        filtered_batches.push(filtered_batch);
                    }
                } else if *id == batch_id && row_id > 0 {
                    if batch.data.is_some() {
                        if let Some(filtered_batch) = batch.get_filtered_batch_with_limit(row_id)? {
                            filtered_batches.push(filtered_batch);
                        }
                    } else {
                        let rows = self.rows.as_ref().unwrap().get_buffer(row_id);
                        let deletions = &self
                            .batches
                            .values()
                            .last()
                            .expect("batch not found")
                            .deletions;
                        let batch = create_batch_from_rows(rows, schema.clone(), deletions);
                        filtered_batches.push(batch);
                    }
                }
            }

            // TODO(hjiang): Check whether we could avoid IO operation inside of critical section.
            if !filtered_batches.is_empty() {
                // Build a parquet file from current record batches
                let temp_file = tokio::fs::File::create(&file_path).await?;
                let props = WriterProperties::builder()
                    .set_compression(Compression::UNCOMPRESSED)
                    .set_dictionary_enabled(false)
                    .set_encoding(Encoding::PLAIN)
                    .build();
                let mut parquet_writer = AsyncArrowWriter::try_new(temp_file, schema, Some(props))?;
                for batch in filtered_batches.iter() {
                    parquet_writer.write(batch).await?;
                }
                parquet_writer.close().await?;
                data_file_paths.push(DataFileForRead::TemporaryDataFile(
                    file_path.to_string_lossy().to_string(),
                ));
                associated_files.push(file_path.to_string_lossy().to_string());
            }
        }
        Ok(SnapshotReadOutput {
            data_file_paths,
            puffin_file_paths,
            deletion_vectors: deletion_vectors_at_read,
            position_deletes,
            associated_files,
            data_file_cache: Some(self.data_file_cache.clone()),
            table_notifier: Some(self.table_notify.as_ref().unwrap().clone()),
        })
    }
}
